{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Streaming Twitter Data Results in S3 with Athena\n",
    "\n",
    "This notebook is meant to follow after you have a running streaming application that is outputing raw and analyzed data into S3. \n",
    "The notebook will show you how you can use the serverless interactive query service [Amazon Athena](https://aws.amazon.com/athena/) to run standard sql queries against the data directly in S3 without needing to move or transform the data into a database. \n",
    "\n",
    "While we could do the following steps using the Athena console or CLI commanes, here we will show you how you can use the [PyAthena](https://pypi.org/project/PyAthena/) library to connect to Amazon Athena to run SQL queries directly from your Jupyter notebook. We will also import the results into Pandas DataFrames, a common representation for further analytics of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the PyAthena library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries we will be using, and set some configurations\n",
    "\n",
    "*Note: It is important to update the **bucket** variable with the name of the bucket that was created in the previous demo steps using CloudFormation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "from pyathena.pandas_cursor import PandasCursor\n",
    "\n",
    "bucket = '[YOUR BUCKET HERE]'\n",
    "\n",
    "conn = connect(s3_staging_dir='s3://{}/athena-staging/'.format(bucket))\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Athena tables\n",
    "\n",
    "We are going to create Amazon Athena tables for our Twitter data using the PyAthena library. This is also a great place to leverage AWS Glue crawling features in your data lake architectures. The crawlers will automatically discover the data format and data types of your different datasets that live in Amazon S3 (as well as relational databases and data warehouses). More details can be found in the documentation for Crawlers with AWS Glue. \n",
    "\n",
    "We will run the following queries to create the Athena database and tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_create_query = \"CREATE DATABASE socialanalytics;\"\n",
    "\n",
    "cursor.execute(db_create_query)\n",
    "print('Created the socialanalytics database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an external table for the raw tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_table = \"CREATE EXTERNAL TABLE socialanalytics.tweets ( \\\n",
    "    coordinates STRUCT< \\\n",
    "        type: STRING, \\\n",
    "        coordinates: ARRAY< \\\n",
    "            DOUBLE \\\n",
    "        > \\\n",
    "    >, \\\n",
    "    retweeted BOOLEAN, \\\n",
    "    source STRING, \\\n",
    "    entities STRUCT< \\\n",
    "        hashtags: ARRAY< \\\n",
    "            STRUCT< \\\n",
    "                text: STRING, \\\n",
    "                indices: ARRAY< \\\n",
    "                    BIGINT \\\n",
    "                > \\\n",
    "            > \\\n",
    "        >, \\\n",
    "        urls: ARRAY< \\\n",
    "            STRUCT< \\\n",
    "                url: STRING, \\\n",
    "                expanded_url: STRING, \\\n",
    "                display_url: STRING, \\\n",
    "                indices: ARRAY< \\\n",
    "                    BIGINT \\\n",
    "                > \\\n",
    "            > \\\n",
    "        > \\\n",
    "    >, \\\n",
    "    reply_count BIGINT, \\\n",
    "    favorite_count BIGINT, \\\n",
    "    geo STRUCT< \\\n",
    "        type: STRING, \\\n",
    "        coordinates: ARRAY< \\\n",
    "            DOUBLE \\\n",
    "        > \\\n",
    "    >, \\\n",
    "    id_str STRING, \\\n",
    "    timestamp_ms BIGINT, \\\n",
    "    truncated BOOLEAN, \\\n",
    "    text STRING, \\\n",
    "    retweet_count BIGINT, \\\n",
    "    id BIGINT, \\\n",
    "    possibly_sensitive BOOLEAN, \\\n",
    "    filter_level STRING, \\\n",
    "    created_at STRING, \\\n",
    "    place STRUCT< \\\n",
    "        id: STRING, \\\n",
    "        url: STRING, \\\n",
    "        place_type: STRING, \\\n",
    "        name: STRING, \\\n",
    "        full_name: STRING, \\\n",
    "        country_code: STRING, \\\n",
    "        country: STRING, \\\n",
    "        bounding_box: STRUCT< \\\n",
    "            type: STRING, \\\n",
    "            coordinates: ARRAY< \\\n",
    "                ARRAY< \\\n",
    "                    ARRAY< \\\n",
    "                        FLOAT \\\n",
    "                    > \\\n",
    "                > \\\n",
    "            > \\\n",
    "        > \\\n",
    "    >, \\\n",
    "    favorited BOOLEAN, \\\n",
    "    lang STRING, \\\n",
    "    in_reply_to_screen_name STRING, \\\n",
    "    is_quote_status BOOLEAN, \\\n",
    "    in_reply_to_user_id_str STRING, \\\n",
    "    user STRUCT< \\\n",
    "        id: BIGINT, \\\n",
    "        id_str: STRING, \\\n",
    "        name: STRING, \\\n",
    "        screen_name: STRING, \\\n",
    "        location: STRING, \\\n",
    "        url: STRING, \\\n",
    "        description: STRING, \\\n",
    "        translator_type: STRING, \\\n",
    "        protected: BOOLEAN, \\\n",
    "        verified: BOOLEAN, \\\n",
    "        followers_count: BIGINT, \\\n",
    "        friends_count: BIGINT, \\\n",
    "        listed_count: BIGINT, \\\n",
    "        favourites_count: BIGINT, \\\n",
    "        statuses_count: BIGINT, \\\n",
    "        created_at: STRING, \\\n",
    "        utc_offset: BIGINT, \\\n",
    "        time_zone: STRING, \\\n",
    "        geo_enabled: BOOLEAN, \\\n",
    "        lang: STRING, \\\n",
    "        contributors_enabled: BOOLEAN, \\\n",
    "        is_translator: BOOLEAN, \\\n",
    "        profile_background_color: STRING, \\\n",
    "        profile_background_image_url: STRING, \\\n",
    "        profile_background_image_url_https: STRING, \\\n",
    "        profile_background_tile: BOOLEAN, \\\n",
    "        profile_link_color: STRING, \\\n",
    "        profile_sidebar_border_color: STRING, \\\n",
    "        profile_sidebar_fill_color: STRING, \\\n",
    "        profile_text_color: STRING, \\\n",
    "        profile_use_background_image: BOOLEAN, \\\n",
    "        profile_image_url: STRING, \\\n",
    "        profile_image_url_https: STRING, \\\n",
    "        profile_banner_url: STRING, \\\n",
    "        default_profile: BOOLEAN, \\\n",
    "        default_profile_image: BOOLEAN \\\n",
    "    >, \\\n",
    "    quote_count BIGINT \\\n",
    ") ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \\\n",
    "LOCATION 's3://{}/raw';\".format(bucket)\n",
    "\n",
    "cursor.execute(tweets_table)\n",
    "print('Created the socialanalytics.tweets table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the tweets table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * from socialanalytics.tweets limit 500\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities and Sentiment Tables\n",
    "\n",
    "Now we will create tables for the entities and sentiment data that is also stored in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_table = \"CREATE EXTERNAL TABLE socialanalytics.tweet_entities ( \\\n",
    "    tweetid BIGINT, \\\n",
    "    entity STRING, \\\n",
    "    type STRING, \\\n",
    "    score DOUBLE \\\n",
    ") ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \\\n",
    "LOCATION 's3://{}/entities/';\".format(bucket)\n",
    "\n",
    "conn.cursor().execute(entities_table)\n",
    "print('Created the Entities table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_table = \"CREATE EXTERNAL TABLE socialanalytics.tweet_sentiments ( \\\n",
    "    tweetid BIGINT, \\\n",
    "    text STRING, \\\n",
    "    originalText STRING, \\\n",
    "    sentiment STRING, \\\n",
    "    sentimentPosScore DOUBLE, \\\n",
    "    sentimentNegScore DOUBLE, \\\n",
    "    sentimentNeuScore DOUBLE, \\\n",
    "    sentimentMixedScore DOUBLE \\\n",
    ") ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' \\\n",
    "LOCATION 's3://{}/sentiment/';\".format(bucket)\n",
    "\n",
    "conn.cursor().execute(sentiment_table)\n",
    "print('Created the Sentiment table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the data\n",
    "\n",
    "Now that we have our tables created, we can run sql queries against them to begin exploring the data. \n",
    "\n",
    "### Run the following query to view a sample of the sentiments table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * from socialanalytics.tweet_sentiments limit 20;\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull the top entity types:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select type, count(*) cnt from socialanalytics.tweet_entities group by type order by cnt desc\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s now pull 20 positive tweets and see their scores from sentiment analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from socialanalytics.tweet_sentiments where sentiment = 'POSITIVE' limit 20;\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the distribution of languages accross tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = \"select lang, count(*) cnt \\\n",
    "        from socialanalytics.tweets \\\n",
    "        group by lang \\\n",
    "        order by cnt desc\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can pull the top 20 commercial items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select entity, type, count(*) cnt \\\n",
    "        from socialanalytics.tweet_entities \\\n",
    "        where type = 'COMMERCIAL_ITEM' \\\n",
    "        group by entity, type \\\n",
    "        order by cnt desc limit 20;\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also start to query the translation details. Even if I don’t know the German word for shoe, I could easily do the following query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select ts.text, ts.originaltext \\\n",
    "        from socialanalytics.tweet_sentiments ts \\\n",
    "        join socialanalytics.tweets t on (ts.tweetid = t.id) \\\n",
    "        where lang = 'de' and ts.text like '%shoe%'\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results show a tweet talking about shoes based on the translated text: Let’s also look at the non-English tweets that have Kindle extracted through NLP:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select lang, ts.text, ts.originaltext \\\n",
    "            from socialanalytics.tweet_sentiments ts \\\n",
    "            join socialanalytics.tweets t on (ts.tweetid = t.id) \\\n",
    "            where lang != 'en' and ts.tweetid in \\\n",
    "                (select distinct tweetid from socialanalytics.tweet_entities where entity = 'AWS')\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "When you are done with the demo, uncomment and run the following cell to drop all of the tables and the database we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db_delete_query = \"DROP DATABASE socialanalytics CASCADE;\"\n",
    "#conn.cursor().execute(db_delete_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
